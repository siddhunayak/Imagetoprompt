
# ğŸ–¼ï¸ Image-to-Prompt with CLIP Interrogator

This project demonstrates how to **reverse engineer prompts from images** using the [CLIP Interrogator](https://github.com/pharmapsychotic/clip-interrogator).  
It leverages **OpenAIâ€™s CLIP** model to analyze an input image, compare it with thousands of candidate tokens (artists, styles, descriptors), and assemble a descriptive **AI art prompt**.

---

## ğŸš€ Features
- Load any image (e.g., `monalisa.png`).
- Extract image embeddings using **CLIP (ViT-L-14 or others)**.
- Compare against curated vocabularies of **styles, artists, and keywords**.
- Generate descriptive prompts for use with **Stable Diffusion, MidJourney, or other AI art models**.

---

## ğŸ—ï¸ Architecture

### CLIP Foundation
- **Image Encoder (ViT-L-14)** â†’ Converts image â†’ embedding.
- **Text Encoder (Transformer)** â†’ Converts candidate tokens â†’ embeddings.
- Both live in the **same embedding space**.

### CLIP Interrogator Pipeline
1. **Image Preprocessing**  
   Convert image to RGB, resize, normalize.

2. **Configuration (`Config`)**  
   Choose CLIP backbone (`ViT-L-14/openai` recommended).

3. **Interrogator Engine**  
   - Compute image embedding.  
   - Generate text embeddings for candidate tokens (artists, styles, descriptors).  
   - Rank tokens by cosine similarity to the image.  
   - Assemble most likely descriptive **prompt**.  

### Workflow Diagram
```

Input Image
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLIP Image Encoderâ”‚â”€â”€â”€â–º Image Embedding
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Candidate Tokens   â”‚â”€â”€â”€â–º â”‚ CLIP Text Encoder (Embeddings)â”‚
â”‚ (styles, artists,  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ descriptors, etc.) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
Cosine Similarity
â”‚
â–¼
Ranked Tokens
â”‚
â–¼
Assembled Prompt

````

---

## ğŸ“¦ Installation

Clone this repo and install dependencies:

```bash
git clone https://github.com/your-username/image-to-prompt.git
cd image-to-prompt

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install required packages
pip install pillow torch transformers traitlets clip-interrogator
````

---

## â–¶ï¸ Usage

### 1. Import Libraries

```python
from PIL import Image
from clip_interrogator import Config, Interrogator
```

### 2. Load an Image

```python
image = Image.open("monalisa.png").convert("RGB")
image
```

### 3. Initialize Interrogator

```python
ci = Interrogator(Config(clip_model_name="ViT-L-14/openai"))
```

### 4. Generate Prompt

```python
prompt = ci.interrogate(image)
print("Generated Prompt:\n", prompt)
```

---

## ğŸ“‚ Example Output

**Input Image:**
*`monalisa.png`*

**Generated Prompt:**

```
a renaissance portrait painting of a woman, by Leonardo da Vinci, highly detailed, realistic, artstation
```

---

## ğŸ“– References

* [CLIP Interrogator GitHub](https://github.com/pharmapsychotic/clip-interrogator)
* [OpenAI CLIP Paper](https://arxiv.org/abs/2103.00020)

---

## ğŸ“ License

This project is for educational and research purposes.
Check original CLIP Interrogator repo for full license details.

```

---

ğŸ‘‰ Do you want me to also create a **`requirements.txt`** file alongside this `README.md` so you can directly install all dependencies in one go?
```
